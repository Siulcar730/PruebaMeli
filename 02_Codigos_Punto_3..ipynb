{
  "metadata": {
    "name": "punto_3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import java.io.File\n\nimport org.apache.spark.sql.{Row, SaveMode, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.hadoop.conf.Configuration\nimport scala.collection.mutable\nimport scala.sys.process._\nimport scala.util.Try\nimport org.apache.spark.sql.SQLContext\nimport scala.xml.XML\nimport com.google.gson.Gson\nimport scala.io.Source\nimport scala.language.postfixOps\nimport org.json4s._\nimport org.apache.hadoop.fs._\n//import spark.implicits._\nimport scala.collection.mutable.ListBuffer\nimport org.json4s.jackson.JsonMethods.parse\nimport org.json4s.jackson.JsonMethods.parse\nimport org.json4s.DefaultFormats\n\n\nval warehouseLocation \u003d new File(\"spark-warehouse\").getAbsolutePath\n\nval spark \u003d SparkSession\n    .builder()\n    .appName(\"Punto_3\")\n    .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n    .enableHiveSupport()\n    .getOrCreate()\n    \nval path \u003d \"hdfs://cluster-c9a8-m/preprocesados/\"\nval r \u003d Seq(\"hdfs\", \"dfs\", \"-ls\", path).!!\nval s \u003d r.split(\"\\n\")\nval lista_archivos \u003d s.filter(line \u003d\u003e !line.equals(s.head)).map(line \u003d\u003e line.split(\" +\").last)\nval separador \u003d \",\"\n\n/*Primera parte del punto 3, que se encarga de leer el archivo .csv ubicado en HDFS*/\nval df_datos \u003d spark.read\n        .option(\"header\", \"true\")\n        .option(\"delimiter\", separador)\n        .csv(lista_archivos: _*)\n        \n/*Creacion de DataFrame con la estrcutrua del archivo .csv*/        \nval datos_df \u003d df_datos.select(\n                col(\"_c0\").as(\"id\"),\n                col(\"venta_id\"),\n                col(\"first_name\"),\n                col(\"last_name\"),\n                col(\"book_id\"),\n                col(\"cuantos_libros\").as(\"cnt_libros\"),\n                col(\"ip_address\"),\n                col(\"ccn3(Codigo_pais)\").as(\"ccn3\"))\n\n/*Creacion de una tabla en Hive para la insercion de los datos del archivo datos_compras tipo .csv*/\nsql(\"create table if not exists meli.datos_compras (id STRING, venta_id STRING, first_name STRING, last_name STRING, book_id STRING, cnt_libros STRING, ip_address STRING, ccn3 STRING) USING hive\")\n\n/*Insercion de los datos hacia la tabla en Hive*/\ndatos_df.write\n          .format(\"hive\")\n          .mode(\"Ignore\")\n          .saveAsTable(\"meli.datos_compras\")\n\n\n\n\n/*Segunda parte del proceso para extraer los datos de la rest Api*/\n\nimplicit val formats \u003d DefaultFormats\n\n\n/*Se guarda el dataframe consultado dentro de una lista*/        \n\nval df_myList \u003d sql(\"select cast(ccn3 as int) ccn3 from meli.datos_compras where ccn3 is not null group by cast(ccn3 as int)\")\n\nvar myList \u003d df_myList.select(\"ccn3\").collect().map(_(0)).toList\n\n        \n/*Se crea la tabla en HIVE para guardar los datos consultados en la API de rescountries*/        \nsql(\"create table if not exists meli.countries (ccn3 STRING, common STRING) USING hive\")        \n      \n\n/*Recorre un ciclo para extraer solo la parte json de los datos que requrimos por codigo de pais*/      \n       for ( x \u003c- myList ) yield {\n          val urll \u003d \"https://restcountries.com/v3.1/alpha/\"+s\"$x\"+\"?fields\u003dname,ccn3\"\n          val apii \u003d scala.io.Source.fromURL(urll).mkString\n          val jsonn \u003d \"\"+apii+\"\"\n          val DFF \u003d spark.read.json(spark.createDataset(jsonn :: Nil))\n          val diss \u003d DFF.select(\"ccn3\",\"name.common\")\n                        diss.show(false)\n                        diss.write\n                        .format(\"hive\")\n                        .mode(\"Ignore\")\n                        .saveAsTable(\"meli.countries\")\n      } \n\n\n\n/*Tercera parte del punto 3, se realiza una consulta agrupada del campo book_id para que filtre dentro del archivo XML leido*/          \nval df_id_book \u003d sql(\"select book_id from meli.datos_compras group by book_id\")\n\n/*Se guarda el dataframe consultado dentro de una lista*/\nval id_book_arr \u003d df_id_book.select(\"book_id\").collect().map(_(0)).toList\n\nval conf \u003d new Configuration()\n\n/*Se lee el archivo bookCatalog dentro del HDFS*/\n\nval hdfsPath: Path \u003d new Path(\"hdfs://cluster-c9a8-m/preprocesados/bookCatalog.xml\")\nval fs: FileSystem \u003d hdfsPath.getFileSystem(conf)\nval inputStream: FSDataInputStream \u003d fs.open(hdfsPath)\n\ntry {\n/*Creacion de una tabla en Hive para la insercion de los datos del archivo bookCatalog tipo .xml*/\n    sql(\"create table if not exists meli.books (id STRING, publish_date STRING, price STRING, genre STRING) USING hive\")\n\nval xml_load \u003d XML.load(inputStream)\nval books_id\u003d ((xml_load \\\\ \"catalog\" \\\\ \"book\"))\n  \n for ( y \u003c- id_book_arr ) yield { \n  val cataLib \u003d for {\n      item \u003c- books_id\n      if (item \\ \"@id\").text \u003d\u003d s\"$y\"\n    } yield ((item \\\\ \"genre\").text\n      ,(item \\\\ \"price\").text\n      ,(item \\\\ \"publish_date\").text\n      ,(item \\\\ \"@id\").text)\n\nval list \u003d sc.parallelize(List((cataLib(0)._4,cataLib(0)._3,cataLib(0)._2,cataLib(0)._1))).toDF(\"id\",\"publish_date\",\"price\",\"genre\")\n    \n    list.write\n        .format(\"hive\")\n        .mode(\"Ignore\")\n        .saveAsTable(\"meli.books\")\n        }\n\n\n/*Se crea una tabla tipo fact llamada tbl_fct_datos con los datos de tipo transaccion para ser validados con calculos de tipo agregacion*/\n\nsql(\"create table if not exists meli.tbl_fct_datos as SELECT venta_id, cnt_libros, publish_date, price, genre, common from meli.datos_compras d join meli.books b on d.book_id \u003d b.id join meli.countries c on cast(d.ccn3 as int)\u003dc.ccn3\")\n\nval df_fact \u003d sql(\"select count(venta_id) venta_id, sum(cnt_libros) cnt_libs, publish_date, sum(cast(price as int)) price, genre, common as pais from meli.tbl_fct_datos group by publish_date, genre, common\")\n\ndf_fact.show()\n\n    \n}finally {\n/*cierra recursos*/ \n  inputStream.close()\n  fs.close()\n}"
    }
  ]
}